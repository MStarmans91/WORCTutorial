{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORC Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the tutorial of WORC: a Workflow for Optimal Radiomics Classification! It will provide you with basis knowledge and practical skills on how to run the WORC. For advanced topics and WORCflows, please see the *WORCAdvanced*\n",
    "iPython notebook also provide with this Tutorial.\n",
    "\n",
    "Besides this tutorial, a special Virtual Machine (VM) has been made on which many components are pre-installed. Hence, for many parts of this tutorial, instructions are split for when using the VM or not.\n",
    "\n",
    "This tutorial consists of the following components:\n",
    "1. WORC Installation and Configuration\n",
    "2. WORC Example\n",
    "\n",
    "\n",
    "For any questions or tips, please contact me at m.starmans@erasmusmc.nl!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORC Installation and Configuration\n",
    "Then using the VM, all components required for WORC are already pre-installed. When not using the VM, please use the installation *installation.sh* script for installing the required components.\n",
    "\n",
    "WORC makes use of [fastr](https://fastr.readthedocs.io/en/stable/), a Python package for standardizing workflows. Fastr has extensive documentation, but I will highlight several important components. However, I at least recommend you to read the fastr introduction.\n",
    "\n",
    "### 1. The actual [fastr configuration file](https://fastr.readthedocs.io/en/stable/static/file_description.html#config-file)\n",
    "The fastr configuration file can be stored in the ~/.fastr hidden folder as config.py. It is just a Python file and is thus also formatted in such a way. Note that upon installation, default settings are used and the file is not actually created. Note that you can have multiple configuration files which can stack: these have to be stored in the ~/.fastr/config.d folder. For WORC and PREDICT, configuration files have been added in that folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the fastr configuration in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]  __init__:0077 >> Not running in a production installation (branch \"unknown\" from installed package)\n",
      "# [bool] Flag to enable/disable debugging\n",
      "debug = False\n",
      "\n",
      "# [str] Directory containing the fastr examples\n",
      "examplesdir = \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\examples\"\n",
      "\n",
      "# [str] The default execution plugin to use\n",
      "execution_plugin = \"ProcessPoolExecution\"\n",
      "\n",
      "# [str] Execution script location\n",
      "executionscript = \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\execution\\\\executionscript.py\"\n",
      "\n",
      "# [str] Redis url e.g. redis://localhost:6379\n",
      "filesynchelper_url = \"\"\n",
      "\n",
      "# [str] The level of cleanup required, options: all, no_cleanup, non_failed\n",
      "job_cleanup_level = \"no_cleanup\"\n",
      "\n",
      "# [bool] Indicate if default logging settings should log to files or not\n",
      "log_to_file = False\n",
      "\n",
      "# [str] Directory where the fastr logs will be placed\n",
      "logdir = \"C:\\\\Users\\\\Marty\\\\.fastr\\\\logs\"\n",
      "\n",
      "# [dict] Python logger config\n",
      "logging_config = {}\n",
      "\n",
      "# [int] The log level to use (as int), INFO is 20, WARNING is 30, etc\n",
      "loglevel = 20\n",
      "\n",
      "# [str] Type of logging to use\n",
      "logtype = \"default\"\n",
      "\n",
      "# [dict] A dictionary containing all mount points in the VFS system\n",
      "mounts = {\n",
      "  \"tmp\": \"C:\\\\Users\\\\Marty\\\\AppData\\\\Local\\\\Temp\",\n",
      "  \"example_data\": \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\examples\\\\data\",\n",
      "  \"home\": \"C:\\\\Users\\\\Marty/\",\n",
      "  \"worc_example_data\": \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\WORC\\\\exampledata\",\n",
      "  \"apps\": \"C:\\\\Users\\\\Marty\\\\apps\",\n",
      "  \"output\": \"C:\\\\Users\\\\Marty\\\\WORC\\\\output\",\n",
      "  \"test\": \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\WORC\\\\resources\\\\fastr_tests\"\n",
      "}\n",
      "\n",
      "# [list] Directories to scan for networks\n",
      "networks_path = [\n",
      "  \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\resources\\\\networks\"\n",
      "]\n",
      "\n",
      "# [list] Directories to scan for plugins\n",
      "plugins_path = [\n",
      "  \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\resources\\\\plugins\"\n",
      "]\n",
      "\n",
      "# [list] A list indicating the order of the preferred types to use. First item is most preferred.\n",
      "preferred_types = [\n",
      "  \"NiftiImageFileCompressed\"\n",
      "]\n",
      "\n",
      "# [list] A list of modules in the environmnet modules that are protected against unloading\n",
      "protected_modules = []\n",
      "\n",
      "# [list] The reporting plugins to use, is a list of all plugins to be activated\n",
      "reporting_plugins = [\n",
      "  \"SimpleReport\"\n",
      "]\n",
      "\n",
      "# [str] Directory containing the fastr system resources\n",
      "resourcesdir = \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\resources\"\n",
      "\n",
      "# [str] Directory containing the fastr data schemas\n",
      "schemadir = \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\resources\\\\schemas\"\n",
      "\n",
      "# [int] The number of source jobs allowed to run concurrently\n",
      "source_job_limit = 0\n",
      "\n",
      "# [str] Fastr installation directory\n",
      "systemdir = \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\"\n",
      "\n",
      "# [list] Directories to scan for tools\n",
      "tools_path = [\n",
      "  \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\WORC\\\\resources\\\\fastr_tools\",\n",
      "  \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\resources\\\\tools\"\n",
      "]\n",
      "\n",
      "# [list] Directories to scan for datatypes\n",
      "types_path = [\n",
      "  \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\WORC\\\\resources\\\\fastr_types\",\n",
      "  \"C:\\\\Users\\\\Marty\\\\Anaconda3\\\\envs\\\\py372\\\\lib\\\\site-packages\\\\fastr-3.0.2-py3.7.egg\\\\fastr\\\\resources\\\\datatypes\"\n",
      "]\n",
      "\n",
      "# [str] Fastr user configuration directory\n",
      "userdir = \"C:\\\\Users\\\\Marty\\\\.fastr\"\n",
      "\n",
      "# [bool] Warning users on import if this is not a production version of fastr\n",
      "warn_develop = True\n",
      "\n",
      "# [str] The hostname to expose the web app for\n",
      "web_hostname = \"localhost\"\n",
      "\n",
      "# [int] The port to expose the web app on\n",
      "web_port = 5000\n",
      "\n",
      "# [str] The secret key to use for the flask web app\n",
      "web_secret_key = \"VERYSECRETKEY!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fastr\n",
    "\n",
    "print(fastr.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a *~/.fastr/config.py* file for you on the VM, in which we specified two fields:\n",
    "\n",
    "job_cleanup_level = 'no_cleanup'\n",
    "execution_plugin = 'LinearExecution'\n",
    "\n",
    "**Please also do so if you are not using the VM**. More details on this setting will be given in the Exectution of a WORC example later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mounts\n",
    "Fastr defines several *mounts*, i.e. paths that are referred to by a specific name. These are very useful to run pipelines cross platform by using the Virtual File System (VFS) plugin, which will be detailed in the WORC Example. For example, suppose you define the mount *Data* as */home/worc/Documents/Data* on this machine and as */home/yourname/somepath* on another machine. When storing the file *image.nii.gz* on path machines, you can run a pipeline that uses *vfs: mount['home']/image.nii.gz* on both machines without needing any adjustments.\n",
    "\n",
    "WORC makes use of several of these mounts, namely *worc_example_data, apps, output* and *test*. These have already been defined for you in the *~/.fastr/config.d/WORC_config.py* file. **Important Note**: the site package is used to automatically locate your installation folder. This does not work in a virtual environment. I have tried to circumvent this, but please check the config file to see if the packagedir is actually referring to the directory where your Python packages for the  virtual environment are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mounts can also be found in the fastr config, where you can see the mounts we previously mentioned for WORC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fastr.config.mounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORC Example\n",
    "\n",
    "It's time to create and run your first WORC Example! We will use open source data from the [BMIA XNAT](https://xnat.bmia.nl/) platform. XNAT is an open source, online platform designed for storing, sharing and structuring medical image data. The BMIA XNAT is an incentive from the Netherlands to create an online biobank. Although most datasets are private and therefore not accessible, there are several public datasets. We will make use of the [STW Strategy Multidelination set](https://xnat.bmia.nl/app/template/XDATScreen_report_xnat_projectData.vm/search_element/xnat:projectData/search_field/xnat:projectData.ID/search_value/stwstrategymmd), which consists of CT and PET scans of patients with lung cancer. More detail can be found in the following paper:\n",
    "\n",
    "\n",
    "Aerts, H. J. W. L., Velazquez, E. R., Leijenaar, R. T. H., Parmar, C., Grossmann, P., Carvalho, S., … Lambin, P. (2014, June 3). Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach. Nature Communications. Nature Publishing Group. http://doi.org/10.1038/ncomms5006\n",
    "\n",
    "Feel free to use images of your own instead! **NOTE:** the pipeline execution setup for this notebook is set to be rather     slow to save computing space. Hence we suggest you start using only a handfull of patients (e.g. 10) to run this notebook. \n",
    "\n",
    "We will first download the images using the XNATpy package, convert them to Nifti's and create very simplistic masks of the patient. You do not need to understand these steps at in order to use WORC, just execute them to get the correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading patient interobs05, experiment interobs05_20190220_CT, scan 1_3_6_1_4_1_9590_100_1_2_170217758912108379426621313680109428629.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64.0 KiB |                    #            |  31.3 KiB/s Elapsed Time: 0:00:02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e8ae5b3718f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downloading patient {}, experiment {}, scan {}.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                     \u001b[0mscan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DICOM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Disconnect the session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\tmpbqo4xdhw_generated_xnat.py\u001b[0m in \u001b[0;36mdownload_dir\u001b[1;34m(self, target_dir, verbose)\u001b[0m\n\u001b[0;32m   3525\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3526\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtemp_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3527\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxnat_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/files'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3529\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\xnat\\session.py\u001b[0m in \u001b[0;36mdownload_stream\u001b[1;34m(self, uri, target_stream, format, verbose, chunk_size, update_func, timeout)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[0mupdate_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbytes_read\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'<'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<!DOCTYPE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'<html>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid response from XNATSession (status {}):\\n{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    667\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m                 decoded = self._decode(chunk, decode_content=decode_content,\n\u001b[0;32m    671\u001b[0m                                        flush_decoder=False)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    622\u001b[0m             \u001b[0mreturned_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# amt > self.chunk_left\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m             \u001b[0mreturned_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Toss the CRLF at the end of the chunk.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Unexpected EOF'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1819\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1821\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1822\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xnat\n",
    "import os\n",
    "\n",
    "# Locate your home folder\n",
    "home = os.path.expanduser('~')\n",
    "\n",
    "## Download the images\n",
    "# We will only download patients with these labels for the moment\n",
    "subject_labels = ['interobs' + num for num in ['05', '06', '08', '09', '10', '11', '12', '13', '15']]\n",
    "\n",
    "# Connect to XNAT and retreive project\n",
    "session = xnat.connect('https://xnat.bmia.nl/')\n",
    "project = session.projects['stwstrategymmd']\n",
    "\n",
    "# Create the data folder if it does not exist yet\n",
    "datafolder = os.path.join(home, 'Documents', 'Data', 'STWStrategyMMD')\n",
    "if not os.path.exists(datafolder):\n",
    "    os.makedirs(datafolder)\n",
    "\n",
    "# Download the data\n",
    "for s in subject_labels:\n",
    "    subject = project.subjects[s]\n",
    "    for e in subject.experiments:\n",
    "        experiment = subject.experiments[e]\n",
    "        # NOTE: We only download the CT sessions, no PET scans\n",
    "        if '_CT' in experiment.session_type:\n",
    "            # NOTE: We only download the images, not the RTStruct file, which is a scan consisting of a single file\n",
    "            for s in experiment.scans:\n",
    "                scan = experiment.scans[s]\n",
    "                if len(scan.files) > 1:\n",
    "                    print(('Downloading patient {}, experiment {}, scan {}.').format(subject.label, experiment.label, scan.id))\n",
    "                    scan.resources['DICOM'].download_dir(datafolder)\n",
    "                    \n",
    "# Disconnect the session\n",
    "session.disconnect()\n",
    "\n",
    "print('Done downloading!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions for the conversion and mask creation: you do not need to follow this.\n",
    "import dicom as pydicom\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def mmdpreprocess(image_in, image_out, segmentation_out):\n",
    "        '''\n",
    "        Converts input image DICOM folder to output nifti and segmentation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        WIP\n",
    "\n",
    "        '''\n",
    "        # Convert input arguments from list to arguments\n",
    "        if type(image_in) is list:\n",
    "            image_in = ''.join(image_in)\n",
    "\n",
    "        if type(image_out) is list:\n",
    "            image_out = ''.join(image_out)\n",
    "\n",
    "        if type(segmentation_out) is list:\n",
    "            segmentation_out = ''.join(segmentation_out)\n",
    "\n",
    "        # Load the DICOMs from the folder\n",
    "        print('Loading DICOM')\n",
    "        image, _ = load_image(image_in)\n",
    "\n",
    "        # We make a dummy segmentation by simply selecting a cube in the image\n",
    "        # Note that we use a random volume of either 10 or 20 for half width.\n",
    "        print('Creating mask.')\n",
    "        width = random.choice([10, 20])\n",
    "        x, y, z = image.shape\n",
    "        mask = np.zeros(image.shape)\n",
    "        mask[int(x)/2 - width:int(x)/2 + width,\n",
    "             int(y)/2 - width:int(y)/2 + width,\n",
    "             int(z)/2 - width:int(z)/2 + width] = 1\n",
    "\n",
    "        # Save image and mask\n",
    "        print('Saving image and segmentation to Nifti.')\n",
    "        image = sitk.GetImageFromArray(image)\n",
    "        mask = sitk.GetImageFromArray(mask)\n",
    "        sitk.WriteImage(image, image_out)\n",
    "        sitk.WriteImage(mask, segmentation_out)\n",
    "\n",
    "\n",
    "def load_image(input_dir):\n",
    "    '''\n",
    "    Load DICOMs from input_dir to a single 3D image and make sure axial\n",
    "    direction is on third axis.\n",
    "    '''\n",
    "    dicom_reader = sitk.ImageSeriesReader()\n",
    "    dicom_file_names = dicom_reader.GetGDCMSeriesFileNames(str(input_dir))\n",
    "    dicom_reader.SetFileNames(dicom_file_names)\n",
    "    metadata = pydicom.read_file(dicom_file_names[0])\n",
    "    dicom_image = dicom_reader.Execute()\n",
    "    dicom_image = sitk.GetArrayFromImage(dicom_image)\n",
    "\n",
    "    dicom_image = np.transpose(dicom_image, (2,1,0))\n",
    "    dicom_image = np.fliplr(np.rot90(dicom_image, 3))\n",
    "    dicom_image = dicom_image[:, :, ::-1]\n",
    "    return dicom_image, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert images to Nifti\n",
    "import shutil\n",
    "import glob\n",
    "pfolders = glob.glob(datafolder + '/*')\n",
    "for pfolder in pfolders:\n",
    "    \n",
    "    # The DICOMS are often in a subfolder\n",
    "    subfolders = glob.glob(pfolder + '/*')\n",
    "    while len(subfolders) == 1:\n",
    "        imfolder = subfolders[0]\n",
    "        subfolders = glob.glob(imfolder + '/*')\n",
    "    \n",
    "    print(('Processing patient {}.').format(os.path.basename(pfolder)))\n",
    "    image_out = os.path.join(pfolder, 'image.nii.gz')\n",
    "    segmentation_out = os.path.join(pfolder, 'mask.nii.gz')\n",
    "    mmdpreprocess(imfolder, image_out, segmentation_out)\n",
    "    \n",
    "    # Remove the folder with the DICOMS, but save one for later use\n",
    "    dicoms = glob.glob(imfolder + '/*.dcm')\n",
    "    os.rename(dicoms[0], os.path.join(pfolder, 'metadata.dcm'))\n",
    "    shutil.rmtree(os.path.join(pfolder, 'scans'))\n",
    "    \n",
    "print('Done preprocessing patients!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start using WORC\n",
    "\n",
    "Now it's finally time to create your first WORC network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WORC\n",
    "import fastr\n",
    "\n",
    "# Create a network with the name \"Tutorial\", which will be used upon execution\n",
    "network = WORC.WORC('Tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORC has several sources and components you can set and use. Let's start with a minimal example only using the following components:\n",
    "1. Images\n",
    "2. Segmentations\n",
    "3. Labels\n",
    "4. The configuration generator\n",
    "\n",
    "More components and sources are discussed in the *WORCAdvanced* iPython notebook.\n",
    "\n",
    "In general, all sources in WORC are Python lists. You can add multiple types of the same source by simply appending to these listst. For example, when using multiple images per patient, i.e. a T2 MR and a CT scan, you can simply add them by executing network.images.append(MR) and network.images.append(CT). \n",
    "\n",
    "Additionally, there are often two types of each source, one for training and one for testing, e.g. image_train and images_test. These correspond to two workflows:\n",
    "* When only suppling training sources, cross validation will be used to train estimators and estimate performance.\n",
    "* When supplying both training and testing sources, training and testing will be done on these separate sets without using a cross validation.\n",
    "\n",
    "We will use the first option in this example.\n",
    "\n",
    "WORC will automatically adjust the pipeline for the types and number of sources you supply!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images and fastr IOPlugins\n",
    "Images are used to extract features from. Images can be in any image type that the Insight Segmentation and Registration Toolkit (ITK) supports, e.g. Nifti, Nifti Compressed, TIFF, MHD, RAW. Dicom folders are not supported but we do supply a tool for easy conversion, which is found in the *Advanced* notebook. Internally, WORC will convert everything to compressed Nifti, since this takes a lot less memory to process in the other parts of the pipeline.\n",
    "\n",
    "I have already downloaded the CT image ten of the patients from the Multidelination dataset, which can be found in the *Data* folder of the VM, i.e. */home/worc/Documents/WORCTutorial/Data/STWStrategyMMD*, or can be fetched from WIP\n",
    "\n",
    "#### IOPlugins\n",
    "Fastr provides several [IOPlugins](https://fastr.readthedocs.io/en/stable/fastr.reference.html#ioplugin-reference) to import and export the data to fastr. When providing a source or sink, you must mention which plugin has to be used. The most straightforward way to point to this file would be to use FileSystem (called *file*), which corresonds to simply referring to your local file, e.g. for the image of patient *interobs05*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_patient05 = os.path.join('file://home',\n",
    "                                      'Documents',\n",
    "                                      'Data',\n",
    "                                      'STWStrategyMMD',\n",
    "                                      'interobs05_20170910_CT',\n",
    "                                      'image.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when transporting your WORC pipeline to another system, you would have to redefine all your sources. Hence, I advice to always use the VirtualFileSystem (called *vfs*). This uses the mounts described in the installation section of this notebook to refer to specific paths. The */home/worc* folder is already defined as the *home* mount as we saw there, hence we can use that now to use the VFS to refer to our source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_patient05 = os.path.join('vfs://home',\n",
    "                                      'Documents',\n",
    "                                      'Data',\n",
    "                                      'STWStrategyMMD',\n",
    "                                      'interobs05_20170910_CT',\n",
    "                                      'image.nii.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the first string after the *vfs* command is used to define the mount used. Let us use the VFS IOPlugin for fastr to expand this input string or URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastr.ioplugins # Load the plugins\n",
    "VFSPlugin = fastr.ioplugins['vfs']\n",
    "expanded_url = VFSPlugin.url_to_path(source_image_patient05)\n",
    "print('The URL {} is converted by the VFS IOPlugin to {}!').format(source_image_patient05, expanded_url)\n",
    "\n",
    "# Note: don't mind the warnings that some plugins cannot be found, as we will not use those anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several IOPlugins that you can use besides the VFS, see the fastr documentation for more detail. The only other plugin we will discuss is the XNAT plugin, which can be used to directly read data from XNAT, in the Advanced notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image sources\n",
    "\n",
    "We will now use the Python *glob* package to locate all image files and turn them into VFS sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# Locate image sources and convert to VFS sources\n",
    "image_sources = glob.glob(os.path.join(datafolder, '*', 'image.nii.gz'))\n",
    "image_sources = [i.replace(home, 'vfs://home') for i in image_sources]\n",
    "print(image_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources can be supplied to WORC/fastr by either using a list or a dictionary. When providing a list, WORC/fastr will name each sample simply wiht ``sample_0, sample_1, ...``, which is not very informative. Moreover, later on in WORC, the sample ID's will be used to match labels and images of the same patient to each other. Hence, it is **very important** to provide all sources from patients as dictionaries in which the keys include the name in the label file, see later on in this tutorial.\n",
    "\n",
    "Let's therefore convert the list into a dictionary with the correct labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "image_sources = {os.path.basename(os.path.dirname(i)): i for i in image_sources}\n",
    "print(image_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we append them to the WORC object as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.images_train = [image_sources]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentations\n",
    "When using images, currently, segmentations of the ROI from which to extract the features are also required. Another option would be to include a tool that automatically performs the segmentation in the WORCflow: see the *Advanced* notebook on how to add nodes\n",
    "\n",
    "The segmentations can be supplied in the same manner as the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "segmentation_sources = glob.glob(os.path.join(datafolder, '*', 'mask.nii.gz'))\n",
    "segmentation_sources = [i.replace(home, 'vfs://home') for i in segmentation_sources]\n",
    "segmentation_sources = {os.path.basename(os.path.dirname(i)): i for i in segmentation_sources}\n",
    "print(segmentation_sources)\n",
    "network.segmentations_train = [segmentation_sources]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It is very important that you use the same cardinality or keys in the segmentations and images object. WORC/fastr will match each sample of the segmentation source with an image source based on the sample index. Hence, when supplying different keys and therefore a different ordering or lists with different orderings as sources, mismatches will be created. \n",
    "\n",
    "**Note:** Additionally, the number of image sources in the images_train and images_test lists have to match the segmentations in cardinality. Thus, if you supply for example an MR and CT image per patient through network.images.append(MR) and network.images.append(CT), you will also have to supply WORC with a segmentation source for MR and one for CT in that order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "The labels are what your estimator will be trained on and thus what will be predicted for each patient. The labels have to be provided in a single text file containing a table. The first column should head *Patient* and should include the patient ID. When matching the labels to a patient in the estimator training, these labels will be matched to the sample IDs of the images and segmentations. The sample IDs do not exactly have to match, but do need to include the names from the label files. The other columns contain possible labels\n",
    "\n",
    "The label file for this dataset (which contains imaginary labels) can be found in the *WORCTutorial/Data/StrategyMMD/pinfo.txt*. We will now load the file, see how it looks and append it to the WORC labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Patient' 'imaginary_label_1' 'imaginary_label_2']\n",
      " ['interobs05' '1' '0']\n",
      " ['interobs06' '0' '1']\n",
      " ['interobs09' '1' '1']\n",
      " ['interobs10' '0' '0']\n",
      " ['interobs11' '1' '0']\n",
      " ['interobs12' '0' '1']\n",
      " ['interobs13' '1' '1']\n",
      " ['interobs15' '0' '0']]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Make sure you either put the mentioned pinfo.txt\n",
    "# file in the datafolder, or change the fields below to point to the correct paht\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "pinfo_file = os.path.join(datafolder, 'pinfo.txt')\n",
    "pinfo = np.loadtxt(pinfo_file, np.str)\n",
    "print(pinfo)\n",
    "network.labels_train.append(pinfo_file.replace(home, 'vfs://home'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this file contains the labels *imaginary_label_1* and *imaginary_label_2*. In the next section, these names are used in the configuration to tell WORC which label we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The configuration generator\n",
    "Lastly, a configuration file for WORC is mandatory. The config file is a *.ini* file containing specific fields: see the WORC Github Wiki for an explanation of the various fields.\n",
    "\n",
    "A default configuration can be created through the *defaultconfig* function of a WORC network. The resulting config is a ConfigParser object. You can interact with it as a dictionary to set or retreive fields. The config can be saved to a *.ini* file through the write function. Values in the configparser can again be dictionaries.\n",
    "\n",
    "Note however that you can also simply supply the ConfigParser itself to WORC, which will turn it automatically in a *.ini* file upon execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFAULT\n",
      "\n",
      "\n",
      "General\n",
      "--  cross_validation True\n",
      "--  Segmentix False\n",
      "--  PCE False\n",
      "--  FeatureCalculator predict/CalcFeatures:1.0\n",
      "--  Preprocessing worc/PreProcess:1.0\n",
      "--  RegistrationNode 'elastix4.8/Elastix:4.8'\n",
      "--  TransformationNode 'elastix4.8/Transformix:4.8'\n",
      "--  Joblib_ncores 4\n",
      "--  Joblib_backend multiprocessing\n",
      "--  tempsave False\n",
      "\n",
      "\n",
      "Segmentix\n",
      "--  mask subtract\n",
      "--  segtype None\n",
      "--  segradius 5\n",
      "--  N_blobs 1\n",
      "--  fillholes False\n",
      "\n",
      "\n",
      "Normalize\n",
      "--  ROI Full\n",
      "--  Method z_score\n",
      "\n",
      "\n",
      "ImageFeatures\n",
      "--  orientation True\n",
      "--  texture all\n",
      "--  coliage False\n",
      "--  vessel False\n",
      "--  log False\n",
      "--  phase False\n",
      "--  image_type CT\n",
      "--  gabor_frequencies 0.05, 0.2, 0.5\n",
      "--  gabor_angles 0, 45, 90, 135\n",
      "--  GLCM_angles 0, 0.79, 1.57, 2.36\n",
      "--  GLCM_levels 16\n",
      "--  GLCM_distances 1, 3\n",
      "--  LBP_radius 3, 8, 15\n",
      "--  LBP_npoints 12, 24, 36\n",
      "--  phase_minwavelength 3\n",
      "--  phase_nscale 5\n",
      "--  log_sigma 1, 5, 10\n",
      "--  vessel_scale_range 1, 10\n",
      "--  vessel_scale_step 2\n",
      "--  vessel_radius 5\n",
      "\n",
      "\n",
      "Featsel\n",
      "--  Variance True, False\n",
      "--  GroupwiseSearch True\n",
      "--  SelectFromModel False\n",
      "--  UsePCA False\n",
      "--  PCAType 95variance\n",
      "--  StatisticalTestUse False\n",
      "--  StatisticalTestMetric ttest, Welch, Wilcoxon, MannWhitneyU\n",
      "--  StatisticalTestThreshold 0.02, 0.2\n",
      "--  ReliefUse False\n",
      "--  ReliefNN 2, 4\n",
      "--  ReliefSampleSize 1, 1\n",
      "--  ReliefDistanceP 1, 3\n",
      "--  ReliefNumFeatures 25, 200\n",
      "\n",
      "\n",
      "SelectFeatGroup\n",
      "--  shape_features True, False\n",
      "--  histogram_features True, False\n",
      "--  orientation_features True, False\n",
      "--  texture_Gabor_features True, False\n",
      "--  texture_GLCM_features True, False\n",
      "--  texture_GLCMMS_features True, False\n",
      "--  texture_GLRLM_features True, False\n",
      "--  texture_GLSZM_features True, False\n",
      "--  texture_NGTDM_features True, False\n",
      "--  texture_LBP_features True, False\n",
      "--  patient_features False\n",
      "--  semantic_features False\n",
      "--  coliage_features False\n",
      "--  log_features False\n",
      "--  vessel_features False\n",
      "--  phase_features False\n",
      "\n",
      "\n",
      "Imputation\n",
      "--  use False\n",
      "--  strategy mean, median, most_frequent, constant, knn\n",
      "--  n_neighbors 5, 5\n",
      "\n",
      "\n",
      "Classification\n",
      "--  fastr False\n",
      "--  fastr_plugin ProcessPoolExecution\n",
      "--  classifiers SVM\n",
      "--  max_iter 100000\n",
      "--  SVMKernel poly\n",
      "--  SVMC 0, 6\n",
      "--  SVMdegree 1, 6\n",
      "--  SVMcoef0 0, 1\n",
      "--  SVMgamma -5, 5\n",
      "--  RFn_estimators 10, 90\n",
      "--  RFmin_samples_split 2, 3\n",
      "--  RFmax_depth 5, 5\n",
      "--  LRpenalty l2, l1\n",
      "--  LRC 0.01, 1.0\n",
      "--  LDA_solver svd, lsqr, eigen\n",
      "--  LDA_shrinkage -5, 5\n",
      "--  QDA_reg_param -5, 5\n",
      "--  ElasticNet_alpha -5, 5\n",
      "--  ElasticNet_l1_ratio 0, 1\n",
      "--  SGD_alpha -5, 5\n",
      "--  SGD_l1_ratio 0, 1\n",
      "--  SGD_loss hinge, squared_hinge, modified_huber\n",
      "--  SGD_penalty none, l2, l1\n",
      "--  CNB_alpha 0, 1\n",
      "\n",
      "\n",
      "CrossValidation\n",
      "--  N_iterations 100\n",
      "--  test_size 0.2\n",
      "\n",
      "\n",
      "Genetics\n",
      "--  label_names Label1, Label2\n",
      "--  modus singlelabel\n",
      "--  url WIP\n",
      "--  projectID WIP\n",
      "\n",
      "\n",
      "HyperOptimization\n",
      "--  scoring_method f1_weighted\n",
      "--  test_size 0.15\n",
      "--  N_iterations 10000\n",
      "--  n_jobspercore 2000\n",
      "\n",
      "\n",
      "FeatureScaling\n",
      "--  scale_features True\n",
      "--  scaling_method z_score\n",
      "\n",
      "\n",
      "SampleProcessing\n",
      "--  SMOTE True, False\n",
      "--  SMOTE_ratio 1, 0\n",
      "--  SMOTE_neighbors 5, 15\n",
      "--  Oversampling False\n",
      "\n",
      "\n",
      "Ensemble\n",
      "--  Use False\n",
      "\n",
      "\n",
      "FASTR_bugs\n",
      "--  images image.nii.gz\n",
      "--  segmentations mask.nii.gz\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = network.defaultconfig()\n",
    "for k1 in config.keys():\n",
    "    print(k1)\n",
    "    for k2 in config[k1].keys():\n",
    "        print('-- ', k2, config[k1][k2])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of making lots of different configuration files for all tools in WORC, I decided to create a single config file which contains all values and which will be based to all nodes. Hence, the WORC configuration object contains fields specific to WORC, but also to for example the PREDICT toolbox which is mainly used for feature extraction and machine learning.\n",
    "\n",
    "**Note:** The number of configurations you supply to WORC also has to match the number of image source lists/dicts you supply. Thus, if you supply for example an MR and a CT scan per patient through network.images.append(MR) and network.images.append(CT), you will have to supply WORC with two configs. The first configuration will be used for all general settings. For specifics to the image, e.g. the feature extraction, the first configuration will be used for the MR, the second for the CT in this case.\n",
    "\n",
    "**Note:** All values have to be provided as strings in a ConfigParser object. Hence no booleans, integers or whatsoever. The actual tools using these fields are responsible for the correct conversion of these values.\n",
    "\n",
    "Let's stick mostly with the defaults for now. However, as we have only a very small dataset for this example and are not interested in lengthy validation, we turn SMOTE oversampling off, lower the number of crossvalidations to 5, increase the size of the validation set to 30 percent and use Linear Execution as a plugin. Lastly, we will try to predict *imaginary_label_1*. We will then add the configuration file to the WORC network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['SampleProcessing']['SMOTE'] = 'False'\n",
    "config['CrossValidation']['N_iterations'] = '5'\n",
    "config['Genetics']['label_names'] = 'imaginary_label_1'\n",
    "config['HyperOptimization']['test_size'] = '0.3'\n",
    "network.fastr_plugin = 'LinearExecution'\n",
    "network.configs.append(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "You are now ready to execute your first WORC pipeline! Execution consists of three steps. \n",
    "\n",
    "The first is building the network. Based on the sources and configuration you provided, WORC will spawn a pipeline template. If you provided inconsistencies, e.g. 16 patients but only 15 segmentations, WORC will notify you in this stage what's wrong. Additionally, after building the network, you can draw it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vfs://home\\\\Documents\\\\Data\\\\STWStrategyMMD\\\\pinfo.txt']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Network' object has no attribute 'configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dec178cec5a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw_dimension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\WORC\\WORC.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, wtype)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'training'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mwtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'testing'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py372\\lib\\site-packages\\WORC\\WORC.py\u001b[0m in \u001b[0;36mbuild_training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    372\u001b[0m                 \u001b[1;31m# BUG: We currently use the first configuration as general config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m                 \u001b[0mimage_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m                         \u001b[1;31m# Probably, c is a configuration file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Network' object has no attribute 'configs'"
     ]
    }
   ],
   "source": [
    "print(network.labels_train)\n",
    "\n",
    "network.build()\n",
    "network.network.draw_network(network.network.id, draw_dimension=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing is done using the default *draw_network* function from fastr, which uses Graphviz. The image of the pipeline is saved as a *.svg* file in the folder you executed the network. See the fastr documentation for details on the drawing.\n",
    "\n",
    "Next, we set our actual sources correctly and create the outputs (called *sinks* in fastr) through the *set* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution of the network is done through the *execute* function. Note that upon execution, the network is automatically drawn with Graphviz. Fastr has different [plugins to execute your network](http://fastr.readthedocs.io/en/stable/fastr.reference.html#executionplugin-reference). For example, jobs can be submitted in parallel or to separate nodes if you are using a cluster with e.g. SGE or SLURM. We have previously set the execution plugin both in *config.py* and in WORC to *LinearExecution*, which submits the jobs linearly. Although this is rather slow, it will not require the full computing power of your PC, which is fine for this example. In practice, you might want to consider alternative plugins.\n",
    "\n",
    "\n",
    "Note that after running the command below, fastr will keep track of the process in the console, which is also logged. The message can grow quite extensively for large pipelines. Execution will take approximately 15 to 60 minutes, of course depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution process\n",
    "\n",
    "Fastr will create a job for each sample in each node in your network. See the Graphviz image for all nodes in your pipeline and their names. Fastr will first create jobs for the first independent nodes in your network: in this case, the source nodes for the images and the segmentations. These will be queued in the fastr job manager. Due to the *LinearExecution* Execitionplugin that we are using, jobs will be executed linearly. Options for cluster submission and parallel execution are also available, see the fastr documentation.\n",
    "\n",
    "These jobs can have four states:\n",
    "1. Queued\n",
    "2. Failed\n",
    "3. Finished\n",
    "4. Cancelled\n",
    "\n",
    "A job will only be cancelled if that either manually done by the user or if a previous job on which the current job depends has failed.\n",
    "\n",
    "Additionally, fastr will create a temporary directory to write all job information to. These will be done in the directory defined in your networks *fastr_tmpdir* field, which by default is the *tmp* mount. The *tmp* mount is by default */tmp* on Linux machines. Fastr will create a directory with the same name that you gave your network, in this case *Tutorial*. Note that fastr also tells you the tmpdir used upon the start of the execution.\n",
    "\n",
    "Let's see which folders and files are actually present in that folder after the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the temporary directory name to which fastr writes the output\n",
    "tempdir = os.path.join(fastr.config.mounts['tmp'], 'Tutorial')\n",
    "\n",
    "for i in sorted(glob.glob(os.path.join(tempdir, '*'))):\n",
    "    print(os.path.basename(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files are specific for fastr, i.e. the pickles, mostly for job tracking and provenance. See the fastr documentation for more details.\n",
    "\n",
    "We can see that the configuration we made is actually saved here as *.ini* file. Additionationally, there are indeed folders for each node in our network. Remember that we specified the job_cleanup_level parameter in the *config.py* file earlier? This determines whether and how folders are cleaned after execution or not. The default is non_failed, which means that the results and information on jobs that have either not run at all or have finished will be deleted. For illustration purposes, we have set it to no_cleanup to show you all temporary outputs. Please change it back on final execution.\n",
    "\n",
    "If we look in the *images_train_CT_0*, we see the actual samples that were processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(glob.glob(os.path.join(tempdir, 'images_train_CT_0', '*'))):\n",
    "    print(os.path.basename(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the folder of each sample and thus each job of this node, there are several files present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(glob.glob(os.path.join(tempdir, 'images_train_CT_0', 'interobs05_20170910_CT', '*'))):\n",
    "    print(os.path.basename(i))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in sorted(glob.glob(os.path.join(tempdir, 'images_train_CT_0', 'interobs05_20170910_CT', 'result', '*')):\n",
    "    print(os.path.basename(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the default files created by fastr I will adress here:\n",
    "\n",
    "1. The __fastr_prov__.json file: \n",
    "Contains information on the provenance of the result created. This includes the tools used and their versions, but also the sources. If the result of the tool is exported as a sink, this file will also be exported automatically.\n",
    "\n",
    "2. The __fastr_stderr__.txt file:\n",
    "Contains the standard error information of the job. When the job was shut down for reasons such as taking too much memory, this will be listed in this file.\n",
    "\n",
    "3. The __fastr_stdout__.txt file:\n",
    "Contains information on the execution process. This is often the most informative file. It also contains the exact command for the job which was run.\n",
    "\n",
    "Fastr provides several [command line tools](http://fastr.readthedocs.io/en/stable/fastr.commandline.html), which are especially usefull for debugging. Personally however, I simply look in the __fastr_stdout__.txt file for the exact command that was run and rerun that on the command line (identified as the *Calling command* to see what the actual error was from Python, Matlab or whatever program was used for executing the task.\n",
    "\n",
    "Only when a task has finished will the result be written to the result folder. When another task depends on the output of a specific task, this is the file that will be transmitted.\n",
    "\n",
    "**Note:** When you rerun a fastr network in the same temporary directory (which in WORC is the case when you resuse the network's name/id), fastr will smartly look at your results. When tasks have previously already finished, they will not be rerun unless any of the tasks on which it depends has changed. Hence, if you change a source such as an image, all jobs dependent on that image will rerun. **This does not hold for any tools/code you updated.** If you have updated a script or a Python package, this is not automatically detected by fastr. Thus, you would have to manually delete the temporary results for the job to rerun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Output will be written by default to the fastr *output* mount, which is set by WORC as default to *$HOME/WORC/output*. Again, a folder is made for the network ID that you used, *Tutorial* in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfolder = home, 'WORC', 'output', 'Tutorial')\n",
    "\n",
    "for i in sorted(glob.glob(os.path.join(ouputfolder, '*'))):\n",
    "    print(os.path.basename(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that by default, the features, the trained classifier (classification_0.hdf5) and the performance of the classifier are written as output. Each file has an associated *.prov.json* file which states the provenance. The performance is simply a json you can open with a text editor. \n",
    "\n",
    "The features and classifier are stored as *.hdf5* files, which can be loaded with the Python pandas package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "features = pd.read_hdf(os.path.join(outputfolder, 'features', 'features_CT_0_interobs05_20170910_CT_0.hdf5'))\n",
    "\n",
    "# Print the contents of the pandas Dataframe\n",
    "print(features)\n",
    "\n",
    "# Print the feature labels and corresponding values\n",
    "for k, v in zip(features.feature_labels, features.feature_values):\n",
    "    print(k, v)\n",
    "    \n",
    "print(('Total number of features: {}.').format(str(len(features.feature_values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = pd.read_hdf(os.path.join(outputfolder, 'svm_all_0.hdf5'))\n",
    "\n",
    "# Print the contents of the pandas Dataframe\n",
    "print(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume the contents of the feature file are straightforward. The classifiers file contains objects for each label you tried to predict: in this case, only imaginary label_1. There are several fields, which are formatted as lists with an item for each iteration in the cross valition: hence in this case five. The only exceptions are the *config* and *feature_labels* field field.\n",
    "\n",
    "\n",
    "The following fields are present:\n",
    "* Classifiers: a [Randomized Search CV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) from sklearn like object. It actually is a SearchCV object from PREDICT, which is an extensions of the sklearn object\n",
    "* X_train: the features from the patients used for training.\n",
    "* X_test: the features from the patients used for testing.\n",
    "* Y_train: the labels from the patients used for training.\n",
    "* Y_test: the labels from the patients used for testing.\n",
    "* config: the configuration used in all cross validation iterations.\n",
    "* patient_ID_train: the IDs of the patients used for training.\n",
    "* patient_ID_test: the IDs of the patients used for testing.\n",
    "* random_seed: the random seed used for creating the training/validation set split using the sklearn [train_test_split function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "* feature_labels: the labels of the features. The list should be equally long as all feature values from a single patient in the X_train and X_test fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the classifiers in the classifiers file is stored in the performance JSON file, which you can load in Python with the json package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(outputfolder, 'performance_all_0.json'), 'r') as fp:\n",
    "    p = json.load(fp)\n",
    "    \n",
    "print(json.dumps(p, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is by default computed for the single best classifier and ensembles of the best 10 and 50 classifiers. Besides the 95% confidence intervals over all cross validations for several matrics, the patients that were always classified correctly or incorrectly in all iterations where they were located in the test set are identified.\n",
    "\n",
    "For this experiment with only ten patients, dummy masks which are just cubes in the center of the image that don't make sense and imaginary labels, hence the performance is naturally very bad.\n",
    "\n",
    "The performance is generated from the classifiers file by the plot_SVM function form PREDICT (we only use a single classifier in the code below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PREDICT as pr\n",
    "\n",
    "p2 = pr.plotting.plot_SVM.plot_SVM(classifiers, pinfo_file, 'imaginary_label_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that there is some randomness in refitting the classifier. As long as your model is not that dependent on randomness, you use enough cross validations and also larger ensembles, this effect should not be too large. In this case, these factors are not met, hence the result will differ quite a lot everytime you run it. We are working on making this deterministic, but as your model should satisfy the mentioned constraints, it shouldn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End\n",
    "Congratulations: you've successfully executed your first WORC pipeline! Before going to the advanced topics, we suggest you recreate above example with your own data, as then the results will hopefully make more sense. In the WORC_example.py script provided also in the WORCTutorial Github repository, we have condensed all above statements to the lines you actually need to run a WORC framework. Additionally, first look at the [WORC Wiki](https://github.com/MSTarmans91/WORC/wiki) to get a better understanding of parameters, so you can tune them to your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
